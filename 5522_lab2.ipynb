{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5522_lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanosenx86/si-s-e-fivefivetwotwo/blob/lab2/5522_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp2T5S8IQTct",
        "colab_type": "text"
      },
      "source": [
        "#Part 1: A Simple Bayes Net: Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6fYsm5f-UbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HskImt85UhU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TweetUrl='https://github.com/aasiaeet/cse5522data/raw/master/db3_final_clean.csv'\n",
        "tweet_dataframe=pd.read_csv(TweetUrl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScERznWSVBK3",
        "colab_type": "code",
        "outputId": "f5ba403e-cfcf-4093-8105-b105b2e222fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "display(tweet_dataframe.shape)\n",
        "tweet_dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3697, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weight</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>it is very cold out want it to be warmer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7698</td>\n",
              "      <td>dammmmmmm its pretty cold this morning   burr lol</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6146</td>\n",
              "      <td>why does halsey have to be so far away think m...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9356</td>\n",
              "      <td>dammit stop being so cold so can work out</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>its too freakin cold</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weight                                              tweet  label\n",
              "0  1.0000          it is very cold out want it to be warmer      -1\n",
              "1  0.7698  dammmmmmm its pretty cold this morning   burr lol     -1\n",
              "2  0.6146  why does halsey have to be so far away think m...     -1\n",
              "3  0.9356         dammit stop being so cold so can work out      -1\n",
              "4  1.0000                               its too freakin cold     -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Q7tGhlVcOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wordDict maps words to id\n",
        "# X is the document-word matrix holding the presence/absence of words in each tweet\n",
        "wordDict = {}\n",
        "idCounter = 0\n",
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    if word not in wordDict:\n",
        "      wordDict[word] = idCounter\n",
        "      idCounter += 1\n",
        "X = np.zeros((tweet_dataframe.shape[0], idCounter),dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap5o8fzI7rgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(tweet_dataframe.shape[0]):\n",
        "  allWords = tweet_dataframe.iloc[i,1].split(\" \")\n",
        "  for word in allWords:\n",
        "    X[i, wordDict[word]]  = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGm_x8Nm-HL6",
        "colab_type": "code",
        "outputId": "6133321d-c70d-4564-9485-24a64cc8cbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = np.array(tweet_dataframe.iloc[:,2])\n",
        "y[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbVHior7lZFl",
        "colab_type": "code",
        "outputId": "8cee57db-cf7d-4f24-b62b-f7479fe870a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w = np.array(tweet_dataframe.iloc[:,0])\n",
        "w[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.    , 0.7698, 0.6146, 0.9356, 1.    ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E4ILnHU87qW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NaiveBayesClassfier:\n",
        "  def __init__(self):\n",
        "    self.logPriorNegative, self.logPriorPositive = 0.0, 0.0\n",
        "    self.logProbWordAbsentGivenNegative, self.logProbWordAbsentGivenPositive, self.logProbWordPresentGivenNegative, self.logProbWordPresentGivenPositive = np.array([]), np.array([]), np.array([]), np.array([])\n",
        "    self.is_fit=False\n",
        "  # compute three distributions (four variables):\n",
        "  def _compute_distros(self,x,y):\n",
        "    # probWordGivenPositive: P(word|Sentiment = +ive)\n",
        "    probWordGivenPositive=np.sum(x[y>=0,:],axis=0) #Sum each word (column) to count how many times each word shows up (in positive examples)\n",
        "    probWordGivenPositive=probWordGivenPositive/np.sum(y>=0) #Divide by total number of (positive) examples to give distribution\n",
        "\n",
        "    # probWordGivenNegative: P(word|Sentiment = -ive)\n",
        "    probWordGivenNegative=np.sum(x[y<0,:],axis=0)\n",
        "    probWordGivenNegative=probWordGivenNegative/np.sum(y<0)\n",
        "\n",
        "    # priorPositive: P(Sentiment = +ive)\n",
        "    priorPositive = np.sum(y>=0)/y.shape[0] #Number of positive examples vs. all examples\n",
        "    # priorNegative: P(Sentiment = -ive)\n",
        "    priorNegative = 1 - priorPositive\n",
        "    #  (note these last two form one distribution)\n",
        "\n",
        "    return probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative\n",
        "\n",
        "  # compute the following:\n",
        "  # logProbWordPresentGivenPositive\n",
        "  # logProbWordAbsentGivenPositive\n",
        "  # logProbWordPresentGivenNegative\n",
        "  # logProbWordAbsentGivenNegative\n",
        "  # logPriorPositive\n",
        "  # logPriorNegative\n",
        "  def _compute_logdistros(self,distros, min_prob):\n",
        "      #Assume missing words are simply very rare\n",
        "      #So, assign minimum probability to very small elements (e.g. 0 elements)\n",
        "      distros=np.where(distros>=min_prob,distros,min_prob)\n",
        "      #Also need to consider minimum probability for \"not\" distribution\n",
        "      distros=np.where(distros<=(1-min_prob),distros,1-min_prob)\n",
        "      return np.log(distros), np.log(1-distros)\n",
        "\n",
        "  def train(self, x, y):\n",
        "    min_prob = 1/y.shape[0] #Assume very rare words only appeared once\n",
        "    probWordGivenPositive, probWordGivenNegative, priorPositive, priorNegative = self._compute_distros(x, y)\n",
        "    self.logProbWordPresentGivenPositive, self.logProbWordAbsentGivenPositive = self._compute_logdistros(probWordGivenPositive,min_prob)\n",
        "    self.logProbWordPresentGivenNegative, self.logProbWordAbsentGivenNegative = self._compute_logdistros(probWordGivenNegative,min_prob)\n",
        "    self.logPriorPositive, self.logPriorNegative = self._compute_logdistros(priorPositive,min_prob)\n",
        "    # Did this work, or did you get an error?  (Read below.)\n",
        "    # display(self.logProbWordPresentGivenPositive[0:5])\n",
        "    # display(self.logProbWordAbsentGivenPositive[0:5])\n",
        "    # display(self.logProbWordPresentGivenNegative[0:5])\n",
        "    # display(self.logProbWordAbsentGivenNegative[0:5])\n",
        "    # display(self.logPriorPositive, self.logPriorNegative)\n",
        "    self.is_fit = True\n",
        "\n",
        "  def classifyNBWithAbsent(self, words):\n",
        "    # fill in function definition here\n",
        "    if self.is_fit:\n",
        "      pPos = np.dot(1-words, self.logProbWordAbsentGivenPositive) + np.dot(words, self.logProbWordPresentGivenPositive) + self.logPriorPositive\n",
        "      pNeg = np.dot(1-words, self.logProbWordAbsentGivenNegative) + np.dot(words, self.logProbWordPresentGivenNegative) + self.logPriorNegative\n",
        "      pred = pPos-pNeg\n",
        "      return np.sign(pred), np.abs(pred)\n",
        "    raise RuntimeError(\"Error: model has not been trained.\")\n",
        "\n",
        "  def classifyNBWithoutAbsent(self, words):\n",
        "    # fill in function definition here\n",
        "    if self.is_fit:\n",
        "      pPos = np.dot(words, self.logProbWordPresentGivenPositive) + self.logPriorPositive\n",
        "      pNeg = np.dot(words, self.logProbWordPresentGivenNegative) + self.logPriorNegative\n",
        "      pred = pPos-pNeg\n",
        "      return np.sign(pred), np.abs(pred)\n",
        "    raise RuntimeError(\"Error: model has not been trained.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSoB-m1V7umz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(pred, std):\n",
        "  return np.sum(np.equal(pred, std))/float(len(std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdLXcY0PCQX",
        "colab_type": "code",
        "outputId": "987ea5de-9626-433d-b98e-98e18a161ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "display(xTrain.shape, xTest.shape, yTrain.shape, yTest.shape)\n",
        "model = NaiveBayesClassfier()\n",
        "model.train(xTrain, yTrain)\n",
        "print(accuracy((model.classifyNBWithAbsent(xTest))[0], yTest),accuracy((model.classifyNBWithoutAbsent(xTest))[0], yTest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740, 5989)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(2957,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(740,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.8162162162162162 0.827027027027027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR5LHQc3Ns_2",
        "colab_type": "code",
        "outputId": "f081a2a3-e30a-4d4a-b85e-3f43ee18a605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "accAbs, accNoAbs=np.array([]), np.array([])\n",
        "for i in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  model.train(xTrain, yTrain)\n",
        "  accAbs = np.append(accAbs, accuracy((model.classifyNBWithAbsent(xTest))[0], yTest))\n",
        "  accNoAbs = np.append(accNoAbs, accuracy((model.classifyNBWithoutAbsent(xTest))[0], yTest))\n",
        "\n",
        "print(\"Mean accuracy of model with absent words are {0}, the standard deviation is {1}.\".format(np.average(accAbs), np.std(accAbs)))\n",
        "print(\"Mean accuracy of model without absent words are {0}, the standard deviation is {1}.\".format(np.average(accNoAbs), np.std(accNoAbs)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy of model with absent words are 0.8289189189189189, the standard deviation is 0.01078036252712386.\n",
            "Mean accuracy of model without absent words are 0.829864864864865, the standard deviation is 0.011694460614226987.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxfGuD55Q09r",
        "colab_type": "text"
      },
      "source": [
        "The difference between two models is not consistent and the effect size is fairly small, so I cannot tell which is better in general. However, after several trials, on the same train and test sets, the model without considering the absent word usually have a better performance than the model counting the absent words. (difference is usually on ~0.0005 level.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwFUedBUSeIh",
        "colab_type": "text"
      },
      "source": [
        "#Part 2: Include weight factor in Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-Ut17Vijzdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WeightedNaiveBayesClassifier(NaiveBayesClassfier):\n",
        "  def __init__(self):\n",
        "    super(WeightedNaiveBayesClassifier, self).__init__()\n",
        "  \n",
        "  def train(self, weight, x, y):\n",
        "    weighted_x=np.multiply(x.T, weight.T).T\n",
        "    super(WeightedNaiveBayesClassifier, self).train(weighted_x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-nmwDSKluxP",
        "colab_type": "code",
        "outputId": "47de3533-63e5-493f-cc56-68cf1619c37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "wTrain, wTest, xTrain, xTest, yTrain, yTest = train_test_split(w, X, y, test_size = 0.2, random_state=42)\n",
        "model = NaiveBayesClassfier()\n",
        "model.train(xTrain, yTrain)\n",
        "model_w = WeightedNaiveBayesClassifier()\n",
        "model_w.train(wTrain, xTrain, yTrain)\n",
        "print(accuracy((model.classifyNBWithAbsent(xTest))[0], yTest),accuracy((model.classifyNBWithoutAbsent(xTest))[0], yTest))\n",
        "print(accuracy((model_w.classifyNBWithAbsent(xTest))[0], yTest),accuracy((model_w.classifyNBWithoutAbsent(xTest))[0], yTest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8162162162162162 0.827027027027027\n",
            "0.8135135135135135 0.8054054054054054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6ebfaa3a-2546-4b41-9f73-7e76112154d3",
        "id": "Ihjp4rtopShl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "accAbs, accNoAbs, accwAbs, accwNoAbs=np.array([]), np.array([]), np.array([]), np.array([])\n",
        "model = NaiveBayesClassfier()\n",
        "model_w = WeightedNaiveBayesClassifier()\n",
        "for i in range(10):\n",
        "  xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2)\n",
        "  model.train(xTrain, yTrain)\n",
        "  accAbs = np.append(accAbs, accuracy((model.classifyNBWithAbsent(xTest))[0], yTest))\n",
        "  accNoAbs = np.append(accNoAbs, accuracy((model.classifyNBWithoutAbsent(xTest))[0], yTest))\n",
        "  model_w.train(wTrain, xTrain, yTrain)\n",
        "  accwAbs = np.append(accwAbs, accuracy((model_w.classifyNBWithAbsent(xTest))[0], yTest))\n",
        "  accwNoAbs = np.append(accwNoAbs, accuracy((model_w.classifyNBWithoutAbsent(xTest))[0], yTest))\n",
        "\n",
        "print(\"Mean accuracy of unweighted model with absent words = {0}, without absent words = {1}\".format(np.mean(accAbs), np.mean(accNoAbs)))\n",
        "print(\"Mean accuracy of weighted model with absent words = {0}, without absent words = {1}\".format(np.mean(accwAbs), np.mean(accwNoAbs)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy of unweighted model with absent words = 0.8314864864864864, without absent words = 0.8318918918918919\n",
            "Mean accuracy of weighted model with absent words = 0.8268918918918919, without absent words = 0.8297297297297297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eznb4ENxuBXc",
        "colab_type": "text"
      },
      "source": [
        "I simply multipled the weight of each sample with the count in the sample as the new samples. So, a sample with a higher weight will be counted more than those have a smaller weight, so the purpose of weight, measure the importance of each sample, is implemented. However, in my trial, generally speaking, the model performance, both considering the absent words or ignore them, decreased after I applied the weight. My method may not successfully incorperated the weight with corresponding samples. But, the difference between these models are very small (~0.004), I cannot tell if the weight really matters."
      ]
    }
  ]
}
